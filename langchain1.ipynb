{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG for querying scientific paper\n",
    "- data from ScienceDirect \"laser cladding erosion\" 2014-2024 review paper \n",
    "- OpenAI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai api key\n",
    "from langchain_openai import ChatOpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "Chat_model = ChatOpenAI(openai_api_key = api_key)\n",
    "\n",
    "# another example\n",
    "# Chat_model = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pdfs in folder\n",
    "# from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "\n",
    "#from langchain.text_splitter import CharacterTextSplitter \n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "\n",
    "# Create a PyPDFDirectoryLoader\n",
    "#pdf_folder = 'C:\\\\ZZfolder\\\\Research Projects\\\\SAP17PS23\\\\reference\\\\ScienceDirect[laser cladding erosion_2014-2024_review]\\\\'\n",
    "pdf_folder = '.\\\\samplepdf\\\\'\n",
    "loader = PyPDFDirectoryLoader(pdf_folder)\n",
    "\n",
    "#docs = loader.load()\n",
    "\n",
    "# alternatively can load pdf with text splitter\n",
    "# Create a custom text splitter\n",
    "\n",
    "'''\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator='\\n',\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len, \n",
    ")\n",
    "\n",
    "'''\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap = 0,\n",
    "    length_function = len,\n",
    ")\n",
    "\n",
    "# Load and split the documents\n",
    "documents = loader.load_and_split(text_splitter)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain.indexes import VectorstoreIndexCreator\\n\\n\\nindex = VectorstoreIndexCreator().from_loaders([loader])\\n\\nquery = \"What is the best material for improving erosion resistance?\"\\n\\nindex.query(query)\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# From github EnkrateiaLucca/summarizing_and_querying_multiple_pdfs_with_langchain\n",
    "'''\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "\n",
    "index = VectorstoreIndexCreator().from_loaders([loader])\n",
    "\n",
    "query = \"What is the best material for improving erosion resistance?\"\n",
    "\n",
    "index.query(query)\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding and vector store\n",
    "\n",
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "\n",
    "# embedding = OpenAIEmbeddings(openai_api_key = api_key)  # (openai_api_key = api_key)\n",
    "persist_directory = 'db'\n",
    "vectorstore = Chroma.from_documents(documents, OpenAIEmbeddings(), persist_directory=persist_directory)\n",
    "\n",
    "vectorstore.persist() #save the vectorstore to chromadb\n",
    "\n",
    "#vectorstore = Chroma(persist_directory=persist_dirctory, OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity search\n",
    "#query = 'How to improve erosion resistance?'\n",
    "#docs = vectorstore.similarity_search(query)\n",
    "\n",
    "#len(docs)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example prompt\n",
    "\n",
    "''' \n",
    "You are an expert researcher and writer, tasked with answering any question.\n",
    "Generate a comprehensive and informative, yet concise answer of 250 words or less for the given question \n",
    "based solely on the provided search results (URL and content). \n",
    "You must only use information from the provided search results. Use an unbiased and journalistic tone. \n",
    "Combine search results together into a coherent answer. Do not repeat text. \n",
    "Cite search results using [${number}] notation. Only cite the most relevant results that answer the question accurately.\n",
    "Place these citations at the end of the sentence or paragraph that reference them - do not put them all at the end. \n",
    "If different results refer to different entities within the same name, write separate answers for each entity. \n",
    "If you want to cite multiple results for the same sentence, format it as `[${number1}] [${number2}]`. \n",
    "However, you should NEVER do this with the same number - if you want to cite `number1` multiple times for a sentence, \n",
    "only do `[${number1}]` not `[${number1}] [${number1}]`\\n\\n\n",
    "You should use bullet points in your answer for readability. \n",
    "Put citations where they apply rather than putting them all at the end.\\n\\n\n",
    "If there is nothing in the context relevant to the question at hand, just say \\\"Hmm, I'm not sure.\\\" \n",
    "Don't try to make up an answer.\\n\\n\n",
    "Anything between the following `context` html blocks is retrieved from a knowledge bank, \n",
    "not part of the conversation with the user.\\n\\n<context>\\n    \n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variables=['context', 'question'] messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], template=\"You are an assistant for question-answering tasks. \\nUse the following pieces of retrieved context to answer the question. \\nIf you don't know the answer, just say that you don't know. \\nQuestion: {question} \\nContext: {context} \\nAnswer:\\n\"))]\n"
     ]
    }
   ],
   "source": [
    "# prompt\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are an assistant for question-answering tasks. \n",
    "Use the following pieces of retrieved context to answer the question. \n",
    "If you don't know the answer, just say that you don't know. \n",
    "Question: {question} \n",
    "Context: {context} \n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The alloy mixed with 718 for in situ alloying was AlCoCrFeNi high entropy alloy.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "# chat model: Chat_model = ChatOpenAI(openai_api_key = api_key)\n",
    "rag_chain = (\n",
    "    {\"context\": retriever,  \"question\": RunnablePassthrough()} \n",
    "    | prompt \n",
    "    | Chat_model\n",
    "    | StrOutputParser() \n",
    ")\n",
    "\n",
    "rag_chain.invoke('What alloy was mixed with 718 for in situ alloying?')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
